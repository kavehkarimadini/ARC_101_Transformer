{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavehkarimadini/ARC_101_Transformer/blob/main/01_positional_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading text file"
      ],
      "metadata": {
        "id": "GER9FrrhCYg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text.file\n",
        "!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter02/text.txt --output \"text.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSX1AzswtXB9",
        "outputId": "7aa78ac5-0e6d-44cc-9d6c-4282a580e4e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 24151  100 24151    0     0  36393      0 --:--:-- --:--:-- --:--:-- 36371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKJ8Saf6vR9b",
        "outputId": "2f7aa3a0-3771-43b8-dcd3-25147dd001d7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install gensim # Version Gensim 4.0.0 and above\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3MArMQbOOKv",
        "outputId": "4cd581d2-c9a6-4e12-d958-95dd549242cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7o7EeDUUu0Sh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "\n",
        "dprint=0 # prints outputs if set to 1, default=0\n",
        "\n",
        "#‘text.txt’ file\n",
        "sample = open(\"text.txt\", \"r\")\n",
        "s = sample.read()\n",
        "\n",
        "# processing escape characters\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "\n",
        "data = []\n",
        "\n",
        "# sentence parsing\n",
        "for i in sent_tokenize(f):\n",
        "\ttemp = []\n",
        "\t# tokenize the sentence into words\n",
        "\tfor j in word_tokenize(i):\n",
        "\t\ttemp.append(j.lower())\n",
        "\tdata.append(temp)\n",
        "\n",
        "# Creating Skip Gram model\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 512, window = 5, sg = 1)\n",
        "\n",
        "# 1-The 2-black 3-cat 4-sat 5-on 6-the 7-couch 8-and 9-the 10-brown 11-dog 12-slept 13-on 14-the 15-rug.\n",
        "word1='black'\n",
        "word2='brown'\n",
        "pos1=2\n",
        "pos2=10\n",
        "a=model2.wv[word1]\n",
        "b=model2.wv[word2]\n",
        "\n",
        "if(dprint==1):\n",
        "        print(a)\n",
        "\n",
        "# compute cosine similarity\n",
        "dot = np.dot(a, b)\n",
        "norma = np.linalg.norm(a)\n",
        "normb = np.linalg.norm(b)\n",
        "cos = dot / (norma * normb)\n",
        "\n",
        "aa = a.reshape(1,512)\n",
        "ba = b.reshape(1,512)\n",
        "cos_lib = cosine_similarity(aa, ba)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw6UyA0wOaie",
        "outputId": "b8098e46-f312-4d32-848b-4cb985edd157"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cos_lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpVefny6OiHX",
        "outputId": "4f91acec-bf3a-4997-8e93-bce0765183e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99951637]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlTeXmatz7bP"
      },
      "source": [
        "A Positional Encoding example using one line of basic Python using a few lines of code for the sine and cosine functions.\n",
        "I added a Pytorch method inspired by Pytorch.org to explore these methods.\n",
        "The main idea to keep in mind is that we are looking to add small values to the word embedding output so that the positions are taken into account. This means that as long as the cosine similarity, for example, displayed at the end of the notebook, shows the positions have been taken into account, the method can apply. Depending on the Transformer model, this method can be fine-tuned as well as using other methods."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pe1=aa.copy()\n",
        "pe2=aa.copy()\n",
        "pe3=aa.copy()\n",
        "paa=aa.copy()\n",
        "pba=ba.copy()"
      ],
      "metadata": {
        "id": "uifWbziKPNzs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe1[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "835-WXSFPOxN",
        "outputId": "cb64356d-705a-4fab-8692-5c505171200d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512,)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmBUq9MzxQxz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "d_model=512\n",
        "max_print=d_model\n",
        "max_length=20\n",
        "# This line starts a loop that iterates through the dimensions of the word embedding vector\n",
        "#  (pe1, paa). It increments by 2 in each step because it calculates the sine and\n",
        "#  cosine values for even and odd dimensions respectively.\n",
        "for i in range(0, max_print,2):\n",
        "\n",
        "                # pe1[0][i] and pe1[0][i+1] are the elements of the positional encoding vector for the current dimension.\n",
        "                # math.sin and math.cos calculate the sine and cosine values, respectively, based on the position (pos1) of the word and the current dimension (i).\n",
        "                # 10000 ** ((2 * i)/d_model) is the frequency term, which varies for each dimension, allowing the model to capture information about different positions at different scales.\n",
        "\n",
        "                pe1[0][i] = math.sin(pos1 / (10000 ** ((2 * i)/d_model)))\n",
        "                paa[0][i] = (paa[0][i]*math.sqrt(d_model))+ pe1[0][i]\n",
        "                pe1[0][i+1] = math.cos(pos1 / (10000 ** ((2 * i)/d_model)))\n",
        "                paa[0][i+1] = (paa[0][i+1]*math.sqrt(d_model))+pe1[0][i+1]\n",
        "                if dprint==1:\n",
        "                        print(i,pe1[0][i],i+1,pe1[0][i+1])\n",
        "                        print(i,paa[0][i],i+1,paa[0][i+1])\n",
        "                        print(\"\\n\")\n",
        "\n",
        "#print(pe1)\n",
        "# A  method in Pytorch using torch.exp and math.log :\n",
        "max_len=max_length\n",
        "# This line initializes a PyTorch tensor named pe with zeros.\n",
        "# The tensor has dimensions max_len x d_model, where d_model is the dimensionality\n",
        "# of the word embeddings. This tensor will store the positional encodings.\n",
        "pe = torch.zeros(max_len, d_model)\n",
        "# This line creates a tensor named position that contains a sequence of numbers from 0 to max_len - 1.\n",
        "# The unsqueeze(1) function adds an extra dimension to the tensor, making it a column vector.\n",
        "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "# This line calculates the frequency term used in the positional encoding formula.\n",
        "# It uses the exponential function (torch.exp) and the logarithm function (math.log) to create a\n",
        "# sequence of values that decrease exponentially with increasing dimension.\n",
        "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "#  This line calculates the sine values for even dimensions of the positional encoding.\n",
        "#  It uses the torch.sin function to calculate the sine of the product of the position and div_term tensors.\n",
        "#  The result is assigned to the even-indexed columns of the pe tensor.\n",
        "pe[:, 0::2] = torch.sin(position * div_term)\n",
        "pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#print(pe[:, 0::2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgrXed2FwHDC",
        "outputId": "ec91e809-3a00-4d7f-e605-0c23cb36cad8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "black brown\n",
            "[[0.99951637]] word similarity\n",
            "[[0.8600013]] positional similarity\n",
            "[[0.96224964]] positional encoding similarity\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for i in range(0, max_print,2):\n",
        "                pe2[0][i] = math.sin(pos2 / (10000 ** ((2 * i)/d_model)))\n",
        "                pba[0][i] = (pba[0][i]*math.sqrt(d_model))+ pe2[0][i]\n",
        "\n",
        "                pe2[0][i+1] = math.cos(pos2 / (10000 ** ((2 * i)/d_model)))\n",
        "                pba[0][i+1] = (pba[0][i+1]*math.sqrt(d_model))+ pe2[0][i+1]\n",
        "\n",
        "                if dprint==1:\n",
        "                        print(i,pe2[0][i],i+1,pe2[0][i+1])\n",
        "                        print(i,paa[0][i],i+1,paa[0][i+1])\n",
        "                        print(\"\\n\")\n",
        "\n",
        "print(word1,word2)\n",
        "cos_lib = cosine_similarity(aa, ba)\n",
        "print(cos_lib,\"word similarity\")\n",
        "cos_lib = cosine_similarity(pe1, pe2)\n",
        "print(cos_lib,\"positional similarity\")\n",
        "cos_lib = cosine_similarity(paa, pba)\n",
        "print(cos_lib,\"positional encoding similarity\")\n",
        "\n",
        "if dprint==1:\n",
        "        print(word1)\n",
        "        print(\"embedding\")\n",
        "        print(aa)\n",
        "        print(\"positional encoding\")\n",
        "        print(pe1)\n",
        "        print(\"encoded embedding\")\n",
        "        print(paa)\n",
        "\n",
        "        print(word2)\n",
        "        print(\"embedding\")\n",
        "        print(ba)\n",
        "        print(\"positional encoding\")\n",
        "        print(pe2)\n",
        "        print(\"encoded embedding\")\n",
        "        print(pba)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}